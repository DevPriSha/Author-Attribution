{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Requirement already satisfied: pandas in /Users/devprisha/miniforge3/lib/python3.9/site-packages (1.5.1)\n",
      "Requirement already satisfied: python-dateutil>=2.8.1 in /Users/devprisha/miniforge3/lib/python3.9/site-packages (from pandas) (2.8.2)\n",
      "Requirement already satisfied: pytz>=2020.1 in /Users/devprisha/miniforge3/lib/python3.9/site-packages (from pandas) (2022.1)\n",
      "Requirement already satisfied: numpy>=1.20.3 in /Users/devprisha/miniforge3/lib/python3.9/site-packages (from pandas) (1.23.4)\n",
      "Requirement already satisfied: six>=1.5 in /Users/devprisha/miniforge3/lib/python3.9/site-packages (from python-dateutil>=2.8.1->pandas) (1.16.0)\n"
     ]
    }
   ],
   "source": [
    "!pip install pandas"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [],
   "source": [
    "# import stuff\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "# import matplotlib.pyplot as plt\n",
    "# %matplotlib inline\n",
    "# import seaborn as sns\n",
    "\n",
    "import warnings\n",
    "warnings.filterwarnings(\"ignore\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [],
   "source": [
    "pcf_gen = pd.read_csv(\"./Data/pcf-general.csv\")\n",
    "spl_gen_1 = pd.read_csv(\"./Data/spl-gen-1.csv\")\n",
    "spl_gen_2 = pd.read_csv(\"./Data/spl-gen-2.csv\")\n",
    "spl_gen = pd.concat([spl_gen_1, spl_gen_2], axis=0, ignore_index=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [],
   "source": [
    "authorID = list(set(pcf_gen['AuthorID']).intersection(set(spl_gen['AuthorID'])))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [],
   "source": [
    "# this is our training set, AuthorID is the label and Content is the feature\n",
    "pcf_gen_common = pcf_gen[pcf_gen['AuthorID'].isin(authorID)]\n",
    "pcf_gen_common = pcf_gen_common.drop(['Reactions', 'Attachments'], axis=1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [],
   "source": [
    "# create new txt file with content of each author\n",
    "for i in authorID:\n",
    "    with open('./Data/pcf_gen_common/'+str(i)+'.txt', 'w') as f:\n",
    "        message = pcf_gen_common[pcf_gen_common['AuthorID'] == i]['Content'].str.cat(sep=' ')\n",
    "        # check if message has any character that cannot be encoded\n",
    "        # if yes, remove that character\n",
    "        # if no, write the message to the file\n",
    "\n",
    "        try:\n",
    "            f.write(message)\n",
    "        except UnicodeEncodeError:\n",
    "            message = message.encode('ascii', 'ignore').decode('ascii')\n",
    "            f.write(message)\n",
    "    f.close()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [],
   "source": [
    "# create new txt file with content of each author\n",
    "for i in authorID:\n",
    "    with open('./Data/spl_gen/'+str(i)+'.txt', 'w') as f:\n",
    "        message = spl_gen[spl_gen['AuthorID'] == i]['Content'].str.cat(sep=' ')\n",
    "        # check if message has any character that cannot be encoded\n",
    "        # if yes, remove that character\n",
    "        # if no, write the message to the file\n",
    "\n",
    "        try:\n",
    "            f.write(message)\n",
    "        except UnicodeEncodeError:\n",
    "            message = message.encode('ascii', 'ignore').decode('ascii')\n",
    "            f.write(message)\n",
    "    f.close()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Train data: pcf_gen_common\n",
    "# Test data: spl_gen\n",
    "\n",
    "# import stuff\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "# import matplotlib.pyplot as plt\n",
    "# %matplotlib inline\n",
    "# import seaborn as sns\n",
    "\n",
    "import warnings\n",
    "warnings.filterwarnings(\"ignore\")\n",
    "\n",
    "import os\n",
    "import re\n",
    "import string\n",
    "import nltk\n",
    "from nltk.corpus import stopwords\n",
    "from nltk.stem import PorterStemmer\n",
    "from nltk.tokenize import word_tokenize\n",
    "\n",
    "from sklearn.feature_extraction.text import CountVectorizer\n",
    "from sklearn.naive_bayes import MultinomialNB\n",
    "from sklearn.metrics import accuracy_score\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [],
   "source": [
    "# read the training data\n",
    "\n",
    "# create a list of all the files in the folder\n",
    "files = os.listdir('./Data/pcf_gen_common/')\n",
    "# create a list of all the file names\n",
    "file_names = [file.split('.')[0] for file in files]\n",
    "# create a list of all the file paths\n",
    "file_paths = ['./Data/pcf_gen_common/'+file for file in files]\n",
    "\n",
    "# create a dataframe with file names and file paths\n",
    "df = pd.DataFrame({'AuthorID': file_names, 'File_Path': file_paths})\n",
    "\n",
    "# read the content of each file and store it in a list\n",
    "content = []\n",
    "for file in file_paths:\n",
    "    with open(file, 'r') as f:\n",
    "        content.append(f.read())\n",
    "    f.close()\n",
    "\n",
    "# add the content to the dataframe\n",
    "df['Content'] = content\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [],
   "source": [
    "# read the test data\n",
    "\n",
    "# create a list of all the files in the folder\n",
    "files = os.listdir('./Data/spl_gen/')\n",
    "# create a list of all the file names\n",
    "file_names = [file.split('.')[0] for file in files]\n",
    "# create a list of all the file paths\n",
    "file_paths = ['./Data/spl_gen/'+file for file in files]\n",
    "\n",
    "# create a dataframe with file names and file paths\n",
    "df_test = pd.DataFrame({'AuthorID': file_names, 'File_Path': file_paths})\n",
    "\n",
    "# read the content of each file and store it in a list\n",
    "content = []\n",
    "for file in file_paths:\n",
    "    with open(file, 'r') as f:\n",
    "        content.append(f.read())\n",
    "    f.close()\n",
    "\n",
    "# add the content to the dataframe\n",
    "df_test['Content'] = content\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Feature set: word n-grams\n",
    "# Label set: AuthorID\n",
    "# Model: Naive Bayes\n",
    "# ML Task 1\n",
    "\n",
    "#class balancing of the training data\n",
    "df['AuthorID'].value_counts()\n",
    "\n",
    "\n",
    "# create a list of all the authors\n",
    "authors = list(df['AuthorID'].unique())\n",
    "# create a list of all the authors\n",
    "authors_test = list(df_test['AuthorID'].unique())\n",
    "\n",
    "# # create the feature set for the training data\n",
    "# feature_set = create_feature_set(df)\n",
    "# # create the feature set for the test data\n",
    "# feature_set_test = create_feature_set(df_test)\n",
    "\n",
    "# # create the label set\n",
    "# label_set = df['AuthorID']\n",
    "\n",
    "# # create the vectorizer\n",
    "# vectorizer = CountVectorizer()\n",
    "# # fit the vectorizer to the training data\n",
    "# vectorizer.fit(feature_set)\n",
    "# # transform the training data\n",
    "# X = vectorizer.transform(feature_set)\n",
    "# # transform the test data\n",
    "# X_test = vectorizer.transform(feature_set_test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [],
   "source": [
    "# #resample the training data to balance the classes\n",
    "# df_balanced = pd.DataFrame(columns=['AuthorID', 'File_Path', 'Content'])\n",
    "# for author in authors:\n",
    "#     df_balanced = df_balanced.append(df[df['AuthorID'] == author].sample(n=1000, replace=True), ignore_index=True)\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {},
   "outputs": [],
   "source": [
    "# #resample the test data to balance the classes\n",
    "# df_test_balanced = pd.DataFrame(columns=['AuthorID', 'File_Path', 'Content'])\n",
    "# for author in authors_test:\n",
    "#     df_test_balanced = df_test_balanced.append(df_test[df_test['AuthorID'] == author].sample(n=1000, replace=True), ignore_index=True)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "count    6.700000e+01\n",
       "mean     1.097180e+05\n",
       "std      2.385872e+05\n",
       "min      1.200000e+01\n",
       "25%      3.640000e+02\n",
       "50%      5.845000e+03\n",
       "75%      8.639950e+04\n",
       "max      1.246120e+06\n",
       "Name: Content, dtype: float64"
      ]
     },
     "execution_count": 30,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "#range of length of messages in the training data\n",
    "df['Content'].str.len().describe()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Character n-grams"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 76,
   "metadata": {},
   "outputs": [],
   "source": [
    "# create the feature set using 1000 most frequently ranked character n-grams\n",
    "c_vectorizer = CountVectorizer(analyzer='char', ngram_range=(2,3), max_features=1000)\n",
    "# fit the vectorizer to the training data\n",
    "c_vectorizer.fit(df['Content'])\n",
    "# transform the training data\n",
    "X = c_vectorizer.transform(df['Content'])\n",
    "\n",
    "# create the label set\n",
    "y = df['AuthorID']\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 77,
   "metadata": {},
   "outputs": [],
   "source": [
    "X_test = c_vectorizer.transform(df_test['Content'])\n",
    "y_test = df_test['AuthorID']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 78,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Accuracy:  34.32835820895522 %\n",
      "F1 Score:  0.23712208189820128\n"
     ]
    }
   ],
   "source": [
    "# y = label_set\n",
    "# y_test = df_test['AuthorID']\n",
    "def MNB(X, y, X_test, y_test):\n",
    "    # create the model\n",
    "    model = MultinomialNB()\n",
    "    # fit the model to the training data\n",
    "    model.fit(X, y)\n",
    "    # predict the labels for the test data\n",
    "    y_pred = model.predict(X_test)\n",
    "\n",
    "    # # evaluate the model using accuracy percentage\n",
    "    # print('Accuracy: ', accuracy_score(y_test, y_pred)*100, '%')\n",
    "\n",
    "    # #evaluate the model using f1 score\n",
    "    # from sklearn.metrics import f1_score\n",
    "    # print('F1 Score: ', f1_score(y_test, y_pred, average='weighted'))\n",
    "\n",
    "    # #evaluate the model using precision score\n",
    "    # from sklearn.metrics import precision_score\n",
    "    # print('Precision Score: ', precision_score(y_test, y_pred, average='weighted'))\n",
    "\n",
    "    # #evaluate the model using recall score\n",
    "    # from sklearn.metrics import recall_score\n",
    "    # print('Recall Score: ', recall_score(y_test, y_pred, average='weighted'))\n",
    "\n",
    "    return accuracy_score(y_test, y_pred)*100, f1_score(y_test, y_pred, average='weighted'), precision_score(y_test, y_pred, average='weighted'), recall_score(y_test, y_pred, average='weighted')\n",
    "\n",
    "\n",
    "\n",
    "# # plot the confusion matrix\n",
    "# plt.figure(figsize=(10, 10))\n",
    "# sns.heatmap(cm, annot=True, fmt='d')\n",
    "# plt.ylabel('Actual')\n",
    "# plt.xlabel('Predicted')\n",
    "# plt.show()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 67,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Accuracy:  2.9850746268656714 %\n",
      "F1 Score:  0.01243781094527363\n"
     ]
    }
   ],
   "source": [
    "#Model: KNN\n",
    "\n",
    "from sklearn.neighbors import KNeighborsClassifier\n",
    "from sklearn.metrics import accuracy_score\n",
    "\n",
    "# create the model\n",
    "model = KNeighborsClassifier(n_neighbors=5)\n",
    "# fit the model to the training data\n",
    "model.fit(X, y)\n",
    "# predict the labels for the test data\n",
    "y_pred = model.predict(X_test)\n",
    "\n",
    "# evaluate the model using accuracy percentage\n",
    "print('Accuracy: ', accuracy_score(y_test, y_pred)*100, '%')\n",
    "\n",
    "#evaluate the model using f1 score\n",
    "from sklearn.metrics import f1_score\n",
    "print('F1 Score: ', f1_score(y_test, y_pred, average='weighted'))\n",
    "\n",
    "# Confusion matrix\n",
    "from sklearn.metrics import confusion_matrix\n",
    "cm = confusion_matrix(y_test, y_pred)\n",
    "\n",
    "# # plot the confusion matrix\n",
    "# plt.figure(figsize=(10, 10))\n",
    "# sns.heatmap(cm, annot=True, fmt='d')\n",
    "# plt.ylabel('Actual')\n",
    "# plt.xlabel('Predicted')\n",
    "# plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 56,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Accuracy:  7.462686567164178 %\n",
      "F1 Score:  0.06105834464043419\n"
     ]
    }
   ],
   "source": [
    "#Model: SVM\n",
    "\n",
    "from sklearn.svm import SVC\n",
    "from sklearn.metrics import accuracy_score\n",
    "\n",
    "# create the model\n",
    "model = SVC(kernel='linear')\n",
    "# fit the model to the training data\n",
    "model.fit(X, y)\n",
    "# predict the labels for the test data\n",
    "y_pred = model.predict(X_test)\n",
    "\n",
    "# evaluate the model using accuracy percentage\n",
    "print('Accuracy: ', accuracy_score(y_test, y_pred)*100, '%')\n",
    "\n",
    "#evaluate the model using f1 score\n",
    "from sklearn.metrics import f1_score\n",
    "print('F1 Score: ', f1_score(y_test, y_pred, average='weighted'))\n",
    "\n",
    "# Confusion matrix\n",
    "from sklearn.metrics import confusion_matrix\n",
    "cm = confusion_matrix(y_test, y_pred)\n",
    "\n",
    "# # plot the confusion matrix\n",
    "# plt.figure(figsize=(10, 10))\n",
    "# sns.heatmap(cm, annot=True, fmt='d')\n",
    "# plt.ylabel('Actual')\n",
    "# plt.xlabel('Predicted')\n",
    "# plt.show()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 68,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Accuracy:  4.477611940298507 %\n",
      "F1 Score:  0.022174840085287847\n"
     ]
    }
   ],
   "source": [
    "#Model: Decision Tree\n",
    "\n",
    "from sklearn.tree import DecisionTreeClassifier\n",
    "from sklearn.metrics import accuracy_score\n",
    "\n",
    "# create the model\n",
    "model = DecisionTreeClassifier()\n",
    "# fit the model to the training data\n",
    "model.fit(X, y)\n",
    "# predict the labels for the test data\n",
    "y_pred = model.predict(X_test)\n",
    "\n",
    "# evaluate the model using accuracy percentage\n",
    "print('Accuracy: ', accuracy_score(y_test, y_pred)*100, '%')\n",
    "\n",
    "#evaluate the model using f1 score\n",
    "from sklearn.metrics import f1_score\n",
    "print('F1 Score: ', f1_score(y_test, y_pred, average='weighted'))\n",
    "\n",
    "# Confusion matrix\n",
    "from sklearn.metrics import confusion_matrix\n",
    "cm = confusion_matrix(y_test, y_pred)\n",
    "\n",
    "# # plot the confusion matrix\n",
    "# plt.figure(figsize=(10, 10))\n",
    "# sns.heatmap(cm, annot=True, fmt='d')\n",
    "# plt.ylabel('Actual')\n",
    "# plt.xlabel('Predicted')\n",
    "# plt.show()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 74,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Accuracy:  16.417910447761194 %\n",
      "F1 Score:  0.1189054726368159\n"
     ]
    }
   ],
   "source": [
    "#Model: Random Forest\n",
    "\n",
    "from sklearn.ensemble import RandomForestClassifier\n",
    "from sklearn.metrics import accuracy_score\n",
    "\n",
    "# create the model\n",
    "model = RandomForestClassifier(n_estimators=100)\n",
    "# fit the model to the training data\n",
    "model.fit(X, y)\n",
    "# predict the labels for the test data\n",
    "y_pred = model.predict(X_test)\n",
    "\n",
    "# evaluate the model using accuracy percentage\n",
    "print('Accuracy: ', accuracy_score(y_test, y_pred)*100, '%')\n",
    "\n",
    "#evaluate the model using f1 score\n",
    "from sklearn.metrics import f1_score\n",
    "print('F1 Score: ', f1_score(y_test, y_pred, average='weighted'))\n",
    "\n",
    "# Confusion matrix\n",
    "from sklearn.metrics import confusion_matrix\n",
    "cm = confusion_matrix(y_test, y_pred)\n",
    "\n",
    "# # plot the confusion matrix\n",
    "# plt.figure(figsize=(10, 10))\n",
    "# sns.heatmap(cm, annot=True, fmt='d')\n",
    "# plt.ylabel('Actual')\n",
    "# plt.xlabel('Predicted')\n",
    "# plt.show()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 75,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Accuracy:  22.388059701492537 %\n",
      "F1 Score:  0.16823124198014452\n"
     ]
    }
   ],
   "source": [
    "#Model: Logistic Regression\n",
    "\n",
    "from sklearn.linear_model import LogisticRegression\n",
    "from sklearn.metrics import accuracy_score\n",
    "\n",
    "# create the model\n",
    "model = LogisticRegression()\n",
    "# fit the model to the training data\n",
    "model.fit(X, y)\n",
    "# predict the labels for the test data\n",
    "y_pred = model.predict(X_test)\n",
    "\n",
    "# evaluate the model using accuracy percentage\n",
    "print('Accuracy: ', accuracy_score(y_test, y_pred)*100, '%')\n",
    "\n",
    "#evaluate the model using f1 score\n",
    "from sklearn.metrics import f1_score\n",
    "print('F1 Score: ', f1_score(y_test, y_pred, average='weighted'))\n",
    "\n",
    "# Confusion matrix\n",
    "from sklearn.metrics import confusion_matrix\n",
    "cm = confusion_matrix(y_test, y_pred)\n",
    "\n",
    "# # plot the confusion matrix\n",
    "# plt.figure(figsize=(10, 10))\n",
    "# sns.heatmap(cm, annot=True, fmt='d')\n",
    "# plt.ylabel('Actual')\n",
    "# plt.xlabel('Predicted')\n",
    "# plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3.9.13",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.13"
  },
  "orig_nbformat": 4,
  "vscode": {
   "interpreter": {
    "hash": "66e1b1079bf3230657dc79817dd1808095653a349bda2b98cc5d248f4f0cac03"
   }
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
